{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde29125",
   "metadata": {},
   "source": [
    "# 딥러닝에서 데이터 전처리 어떻게하는지 보고나서 RNN 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cbccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import 모음\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import koreanize_matplotlib\n",
    "import tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff3fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc38267",
   "metadata": {},
   "source": [
    "# 텐서플로에서 텍스트 전처리\n",
    "1. 토큰화: 문장을 단어/형태소로 쪼개\n",
    "2. 원핫인코딩 : 문자를 벡터화해주는거\n",
    "3. 임베딩(embedding): 벡터화인데 원핫인코딩을 더 축소하는 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa383b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어빈도수 세기\n",
    "docs = ['텍스트의 각 단어를 나눠 토큰화합니다',\n",
    "        '텍스트의 단어로 토큰화 해야 딥러닝에서 인식됩니다',\n",
    "        '토큰화 한 결과는 딥러닝에서 사용할 수 있습니다',\n",
    "        '텍스트 전처리에는 벡터화 원핫인코딩 패딩으로 길이 맞추기 등이 필요합니다',\n",
    "        '딥러닝 쉽지 않네요']\n",
    "# 공백기준 토큰화 비추.\n",
    "# 가장 긴 텍스트 길이에 짧은 str들의 길이(열)를 맞추는 작업."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aacc4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "token=Tokenizer()\n",
    "token.fit_on_texts(docs)\n",
    "print(\"단어 카운트:\", token.word_counts, end=\"\\n\\n\")\n",
    "print(\"문장 카운트:\", token.document_count, end=\"\\n\\n\")\n",
    "print(\"각 단어가 포함된 문장의 수:\", token.word_docs)\n",
    "print(\"각 단어에 매겨진 인ㄷ게스값:\", token.word_index, end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed436ff",
   "metadata": {},
   "source": [
    "'딥러닝 쉽지 않네요' [26, 27, 28]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05222903",
   "metadata": {},
   "source": [
    "### 문장 카운트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c78957",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 문장 카운트\n",
    "x=token.texts_to_sequences(docs)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235ef089",
   "metadata": {},
   "source": [
    "## 가장 긴 벡터의 길이 추출\n",
    "위 리스트는 리스트들을 담은리스트인점->반복문쓰면 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c5ceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#리스트 표현식으로 max값 추출하기 `\n",
    "[len(i) for i in x]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe351543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#리스트 표현식으로 max값 추출하기 ``\n",
    "max([len(i) for i in x])\n",
    "\n",
    "# 가장긴건 9개 토큰으로 구성된 문장인점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1684c412",
   "metadata": {},
   "source": [
    "## 가장 긴 길이의 문장에 맞춰 패딩\n",
    "- 문장의 시작에는 반드시 0이 있어야함\n",
    "- 가장 긴 문장길이 +1의 길이로 패딩\n",
    "- RNN은 입력을 모두 똑같은 길이로 받아야 하기 때문에,\n",
    "길이가 짧은 문장에는 숫자 0을 앞에 붙여서 길이를 맞춰주는 작업이 필요해요.\n",
    "그래서 pad_sequences()를 사용해서 모든 문장을 가장 긴 문장 길이에 맞춰주는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0d3b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_x = pad_sequences(x,max([len(i) for i in x])+1)\n",
    "padded_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae8ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_x.shape # 몇행몇열"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d95352",
   "metadata": {},
   "source": [
    "## 텍스트읽고 긍부정 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf99738",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs2 = ['너무 재미있네요', '최고예요','참 신기한 딥러닝이에요','인공지능 칭찬합니다', '더 ㅏ세히 배우고 싶어요','변화가 너무 빨라요','GPT 성능이 생각보다 별로네요','제미나이보다는 낫죠','유료결재싫어요','나는 차라리 라마를 쓴다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec197e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=np.array([1,1,1,1,1,0,0,1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76945e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각단어에 인덱스 만들고\n",
    "token=Tokenizer()\n",
    "token.fit_on_texts(docs2)\n",
    "# print(\"단어 카운트:\", token.word_counts, end=\"\\n\\n\")\n",
    "# print(\"문장 카운트:\", token.document_count, end=\"\\n\\n\")\n",
    "# print(\"각 단어가 포함된 문장의 수:\", token.word_docs)\n",
    "# print(\"각 단어에 매겨진 인ㄷ게스값:\", token.word_index, end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4787f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터화하기\n",
    "x=token.texts_to_sequences(docs2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1831986",
   "metadata": {},
   "outputs": [],
   "source": [
    "max([len(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd544a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_x=pad_sequences(x,max([len(i) for i in x])+1)\n",
    "padding_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fee6aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩\n",
    "# 내용물의 쉼표를 제거해주는 작업\n",
    "word_size=len(token.word_index)+1\n",
    "word_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0a38b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 과정\n",
    "for i in iter(Embedding(word_size,8)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dfb7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.Sequential()\n",
    "model.add(Embedding(input_dim=word_size, output_dim=8, input_length=max([len(i) for i in padding_x]))\n",
    "model.add(flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9390185",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(padding_x, classes, epochs=20)\n",
    "print(model.evaluate(padding_x, classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067f2129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127fdf35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c474914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84031866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7f5f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbe42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d903cbba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
